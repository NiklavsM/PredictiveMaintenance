{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5- Hypothetical model for the turbine data. \n",
    "\n",
    "Directory is changed to get all the batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Adding all needed imports\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import regularizers\n",
    "from keras import layers\n",
    "from keras.models import Model\n",
    "#print(tf.keras.__version__)\n",
    "#print(tf.__version__)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob,os\n",
    "import pickle\n",
    "import random as rng\n",
    "\n",
    "\n",
    "dir = \"D:\\\\WindData\\\\POD_Request_3541\\\\Project\\\\processed\\\\trimmed\\\\final\"\n",
    "\n",
    "os.chdir(dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of batches as well as loading the list of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"./data/*.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgLabels = pickle.load( open('./labels.p', \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the batch generator. The model calls the generator each batch for the data and labels. The data should be autoshuffled every time a new epoch starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shuffle the list of files so that the batches are in a different order each epoch\n",
    "def shuffle(trainFiles, avgLabels):\n",
    "    newFileList = []\n",
    "    newLabelList = []\n",
    "    for i in range(len(trainFiles)):\n",
    "        index = rng.randint(0, len(trainFiles)-1)\n",
    "        newFileList.append(trainFiles.pop(index))\n",
    "        newLabelList.append(avgLabels.pop(index))\n",
    "    trainFiles = newFileList\n",
    "    avgLabels = newLabelList\n",
    "    return trainFiles, avgLabels\n",
    "\n",
    "#Batch generator. Data and labels are retrieved from the data file and label list respectively. Yield call should make it loop infinitely as the model requires\n",
    "def generator(trainFiles, avgLabels):\n",
    "    num_samples = len(trainFiles)\n",
    "    \n",
    "    while True:\n",
    "        trainFiles, avgLabels = shuffle(trainFiles, avgLabels)\n",
    "        for i in range(num_samples):\n",
    "            batchTrain = np.load(trainFiles[i])\n",
    "            batchLabels = np.asarray(avgLabels[i])\n",
    "            yield batchTrain, batchLabels\n",
    "            \n",
    "train_generator = generator(files, avgLabels) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model creation, compiling and calling the fit function. Sequential isn't used to make it possible to experiment with parallel neural nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 31\n",
    "numRows = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 60, 31)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 55, 150)           28050     \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 300)               405900    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 150)               45150     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 150)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 151       \n",
      "=================================================================\n",
      "Total params: 479,251\n",
      "Trainable params: 479,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "visible = layers.Input(shape=(numRows, width,))\n",
    "\n",
    "cnn = layers.Conv1D(150, 6, activation='relu')(visible)\n",
    "rnn1 = layers.GRU(300, dropout=0.2)(cnn)\n",
    "\n",
    "\n",
    "\n",
    "dense = layers.Dense(150, activation='relu')(rnn1)\n",
    "drop = layers.Dropout(0.2)(dense)\n",
    "out = layers.Dense(1, activation='linear')(drop)\n",
    "\n",
    "model = Model(inputs=visible, outputs=out)\n",
    "\n",
    "\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9),\n",
    "              loss='mae', \n",
    "              metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1700/1700 [==============================] - 496s 292ms/step - loss: 232268.6635 - mae: 232268.7500\n",
      "Epoch 2/5\n",
      "1700/1700 [==============================] - 480s 283ms/step - loss: 206600.8080 - mae: 206600.8125\n",
      "Epoch 3/5\n",
      "1700/1700 [==============================] - 430s 253ms/step - loss: 186168.4979 - mae: 186168.6562\n",
      "Epoch 4/5\n",
      "1700/1700 [==============================] - 418s 246ms/step - loss: 171760.5677 - mae: 171760.2969\n",
      "Epoch 5/5\n",
      "1700/1700 [==============================] - 420s 247ms/step - loss: 164780.8756 - mae: 164780.8906\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, epochs=5, verbose=1, steps_per_epoch=len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
